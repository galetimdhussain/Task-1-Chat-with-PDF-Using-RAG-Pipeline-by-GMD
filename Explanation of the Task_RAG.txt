Explanation of the Task

The goal of this task is to create a **Retrieval-Augmented Generation (RAG) Pipeline** that enables interaction with semi-structured data stored in multiple PDF files. Here’s how the system works, simplified for easy understanding:



1. Purpose and Workflow
The system processes PDF files to:
1. Extract Text and Data: 
   - Pull structured (tables) and semi-structured (text with delimiters like `|`) data from PDFs.
2. Chunk and Embed:
   - Split the extracted content into smaller, manageable chunks.
   - Convert these chunks into numerical vector embeddings for efficient search.
3. Store in a Vector Database:
   - Store the embeddings for fast similarity-based retrieval.
4. Handle Queries:
   - Process natural language queries by converting them into embeddings.
   - Retrieve relevant data chunks using similarity search.
5. Generate Responses:
   - Use a Large Language Model (LLM) to generate clear and factual answers based on the retrieved content.



2. Key Functionalities
 A. Data Ingestion
- What it Does**: Reads and processes PDF files.

  - Uses `PyMuPDF` (`fitz`) to extract raw text.
  - Uses `pdfplumber` to extract structured tables.
-Example:
  - From the provided unemployment data:
    
    Bachelor's | 4.2|
    
    It extracts: `{'Bachelor's': '4.2'}`.

B. Chunking and Embedding
- **What it Does**: Splits large text into smaller chunks and represents them as numerical vectors.
- How:
  - Uses the `sentence-transformers` library to convert text chunks into embeddings.
  - Stores these embeddings in a `faiss` vector database for fast search.

C. Query Handling
- What it Does**: Answers user queries using similarity search and the LLM.

  - Converts the user’s question into an embedding.
  - Searches the database for the most relevant text chunks.
  - Uses these chunks as context for the LLM to generate a complete answer.

D. Specific Data Extraction
- What it Does**: Extracts specific pieces of data like tables or key-value pairs.

  - Extracts tables from specific pages using `pdfplumber`.
  - Dynamically extracts unemployment data by processing text lines containing delimiters like `|`.



3. Example Scenario
 Input:
PDF Content:
1. Page 1:
   
   Degree Level    | Unemployment Rate (%)
   ----------------|----------------------
   High School     | 6.2
   Associate Degree| 4.5
   Bachelor's      | 4.2
   Master's        | 3.5
   Doctorate       | 2.1
   
2. Page 2:
   
   Year    | Manufacturing | Finance | Arts   | Other
   --------|---------------|---------|--------|---------
   2015    | 19%           | 18%     | 4%     | 59%
   2016    | 18.5%         | 18.5%   | 4.5%   | 58.5%
   2017    | 20%           | 19%     | 5%     | 56%
   ```

Query:
- "What is the unemployment rate for Bachelor's degree?"

Output:
1. Query Response:
   
   The unemployment rate for Bachelor's degree is 4.2%.
   

2. Extracted Unemployment Data:
   
   {
       "High School": "6.2",
       "Associate Degree": "4.5",
       "Bachelor's": "4.2",
       "Master's": "3.5",
       "Doctorate": "2.1"
   }

3. Extracted Table from Page 2:
  
   [
       ["Year", "Manufacturing", "Finance", "Arts", "Other"],
       ["2015", "19%", "18%", "4%", "59%"],
       ["2016", "18.5%", "18.5%", "4.5%", "58.5%"],
       ["2017", "20%", "19%", "5%", "56%"]
   ]
   



4. System Highlights for Explanation
- **User Interaction**:
  - The user provides a natural language query about the PDF's content.
  - The system responds with factual, contextual answers by searching and leveraging relevant PDF data.

- AI-Driven Insight:
  - Combines text retrieval and generation for accurate, human-like responses.
  - Uses embeddings to ensure relevance in search results.

- Customizability:
  - Works with any semi-structured data in PDFs (not just unemployment rates or tables).



Simplified Explanation for AI Testers
This project:
1. Reads and processes PDF files to extract useful data.
2. Splits the data into chunks for better handling.
3. Uses machine learning models to search and retrieve the most relevant information based on user questions.
4. Leverages AI to give clear, factual responses.



